{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-1-3 단층 퍼셉트론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초기에는 입력값에 따른 가중치들, 편향의 합을 구하고 이후 계단함수를 통해 출력값 냈다.\n",
    "# 하지만 은닉층이 없는 간단한 단층퍼센트론은 XOR 문제를 풀 수 없었다\n",
    "# XOR과 같은 문제를 해결하기 위해 은닉층을 추가했고 은닉층이 1개 이상있는 것을 Multilayer Perceptron(MLP)이라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# 단층 퍼셉트론은 학습률(0.1)에 따라 가중치를 업데이트하는 SGDClassifier와 같다.(규제도 없음)\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "iris = load_iris()\n",
    "X = iris['data'][:,2:4]\n",
    "y = (iris['target'] == 0).astype(np.int)\n",
    "\n",
    "sgd = SGDClassifier(loss = 'perceptron',penalty=None,eta0=0.1,learning_rate='constant')\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X,y)\n",
    "sgd.fit(X,y)\n",
    "\n",
    "y_pred_per = per_clf.predict([[2,0.5]])\n",
    "y_pred_sgd = sgd.predict([[2,0.5]])\n",
    "\n",
    "# 아무리 확률적으로 뽑아온다고 해도 값이 너무 다르잖아?\n",
    "print(y_pred_per)\n",
    "print(y_pred_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 케라스로 분류용 다층 퍼셉트론 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시퀀스 모델을 만드는 것으로 가장 간단한 케라스 신경망 모델이다. 순서대로 연결된 층을 일렬로 쌓아서 구성한다.\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "# 첫 번째 층을 만든다. Flatten은 입력 이미지를 1차원으로 변환한다. 즉, 데이터를 받으면 X.reshape(-1,1) 을 계산해준다.\n",
    "# 또한, 첫 번째 층이기 때문에 input_shape도 지정해야한다. \n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "\n",
    "# 뉴런 300개를 가진 Dense 은닉층을 추가한다. Dense 층마다 각자 가중치 행렬을 관리한다. \n",
    "# 이 행렬에는 층의 뉴런과 입력 사이의 모든 연결 가중치가 포함된다.\n",
    "# 은닉층 각각의 노드들의 출력층이 300개, 100개, 10개이다. 마지막이 10개인 이유는 class_names의 결과값이 10개이기 때문이다.\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "# 이렇게 할 수 도 있다.\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = [28,28]),\n",
    "    keras.layers.Dense(300, activation = 'relu'),\n",
    "    keras.layers.Dense(100, activation = 'relu'),\n",
    "    keras.layers.Dense(10, activation = 'softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.core.Flatten at 0x7fdb5393b0d0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7fdb53942590>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7fdb53942890>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7fdb53942f10>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Flatten at 0x7fdb5393b0d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위와 같이 모델을 생성한 이후 compile 매서드를 이용해 사용할 손실 함수와 옵티마이저를 지정해야한다.\n",
    "# 비용함수\n",
    "# > 목표 레이블이 정수로만 이루어져있고 클래스가 배타적이라면 sparse_categorical_crossentropy를 사용한다. ex) 0,1,2,3,4,5,6,7,8,9\n",
    "# > 목표 레이블이 one hot encoding이라면 categorical_crossentropy를 사용한다. ex)0,0,0,0,0,1,0,0\n",
    "# > 목표 레이블이 이진분류 라면, binaray_crossentropy 사용. 또한 이진분류이기 때문에 출력층에 softmax가 아닌 sigmoid가 들어간다.\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'sgd',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/30\n",
      "55000/55000 [==============================] - 3s 49us/sample - loss: 0.3611 - accuracy: 0.8733 - val_loss: 0.3722 - val_accuracy: 0.8686\n",
      "Epoch 2/30\n",
      "55000/55000 [==============================] - 3s 50us/sample - loss: 0.3517 - accuracy: 0.8753 - val_loss: 0.3611 - val_accuracy: 0.8712\n",
      "Epoch 3/30\n",
      "55000/55000 [==============================] - 3s 49us/sample - loss: 0.3425 - accuracy: 0.8786 - val_loss: 0.3595 - val_accuracy: 0.8730\n",
      "Epoch 4/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.3321 - accuracy: 0.8811 - val_loss: 0.3561 - val_accuracy: 0.8732\n",
      "Epoch 5/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.3237 - accuracy: 0.8841 - val_loss: 0.3391 - val_accuracy: 0.8764\n",
      "Epoch 6/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.3162 - accuracy: 0.8875 - val_loss: 0.3374 - val_accuracy: 0.8796\n",
      "Epoch 7/30\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.3085 - accuracy: 0.8902 - val_loss: 0.3396 - val_accuracy: 0.8798\n",
      "Epoch 8/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.3022 - accuracy: 0.8925 - val_loss: 0.3380 - val_accuracy: 0.8798\n",
      "Epoch 9/30\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.2962 - accuracy: 0.8933 - val_loss: 0.3195 - val_accuracy: 0.8866\n",
      "Epoch 10/30\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.2902 - accuracy: 0.8964 - val_loss: 0.3310 - val_accuracy: 0.8838\n",
      "Epoch 11/30\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.2847 - accuracy: 0.8979 - val_loss: 0.3177 - val_accuracy: 0.8874\n",
      "Epoch 12/30\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.2785 - accuracy: 0.9010 - val_loss: 0.3140 - val_accuracy: 0.8900\n",
      "Epoch 13/30\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.2722 - accuracy: 0.9029 - val_loss: 0.3169 - val_accuracy: 0.8880\n",
      "Epoch 14/30\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.2682 - accuracy: 0.9039 - val_loss: 0.3077 - val_accuracy: 0.8920\n",
      "Epoch 15/30\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.2634 - accuracy: 0.9051 - val_loss: 0.3051 - val_accuracy: 0.8910\n",
      "Epoch 16/30\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.2595 - accuracy: 0.9062 - val_loss: 0.3103 - val_accuracy: 0.8938\n",
      "Epoch 17/30\n",
      "55000/55000 [==============================] - 3s 49us/sample - loss: 0.2540 - accuracy: 0.9093 - val_loss: 0.3055 - val_accuracy: 0.8944\n",
      "Epoch 18/30\n",
      "55000/55000 [==============================] - 3s 51us/sample - loss: 0.2497 - accuracy: 0.9103 - val_loss: 0.2985 - val_accuracy: 0.8962\n",
      "Epoch 19/30\n",
      "55000/55000 [==============================] - 3s 50us/sample - loss: 0.2455 - accuracy: 0.9122 - val_loss: 0.3108 - val_accuracy: 0.8872\n",
      "Epoch 20/30\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.2415 - accuracy: 0.9135 - val_loss: 0.3109 - val_accuracy: 0.8876\n",
      "Epoch 21/30\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.2379 - accuracy: 0.9137 - val_loss: 0.2924 - val_accuracy: 0.8954\n",
      "Epoch 22/30\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.2327 - accuracy: 0.9171 - val_loss: 0.2996 - val_accuracy: 0.8932\n",
      "Epoch 23/30\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.2297 - accuracy: 0.9181 - val_loss: 0.2931 - val_accuracy: 0.8952\n",
      "Epoch 24/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.2257 - accuracy: 0.9191 - val_loss: 0.3085 - val_accuracy: 0.8860\n",
      "Epoch 25/30\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.2221 - accuracy: 0.9209 - val_loss: 0.3007 - val_accuracy: 0.8924\n",
      "Epoch 26/30\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.2185 - accuracy: 0.9221 - val_loss: 0.3258 - val_accuracy: 0.8858\n",
      "Epoch 27/30\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.2152 - accuracy: 0.9238 - val_loss: 0.3060 - val_accuracy: 0.8910\n",
      "Epoch 28/30\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.2108 - accuracy: 0.9256 - val_loss: 0.2936 - val_accuracy: 0.8938\n",
      "Epoch 29/30\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.2080 - accuracy: 0.9262 - val_loss: 0.2940 - val_accuracy: 0.8982\n",
      "Epoch 30/30\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.2050 - accuracy: 0.9281 - val_loss: 0.2985 - val_accuracy: 0.8926\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30,validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 19us/sample - loss: 0.3297 - accuracy: 0.8828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.32973676719069483, 0.8828]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sequential api를 이용해 만든 모델의 객체인 history는 수행된 에포크가 끝날 때마다 훈련 세트와 검증세크에 대한 손힐과 측정한 지표를 담은 딕셔너리 속성을 갖는다.\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot()\n",
    "plt.gca().set_ylim(0,1)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 19us/sample - loss: 0.3297 - accuracy: 0.8828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.32973676719069483, 0.8828]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99],\n",
       "       [0.  , 0.  , 0.99, 0.  , 0.01, 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_new)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(class_names)[y_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 케라스로 회귀용 다층 퍼셉트론 만들기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full,X_test,y_train_full,y_test = train_test_split(housing['data'],housing['target'])\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full,y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.fit_transform(X_valid)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    keras.layers.Dense(30,activation = 'relu',input_shape = X_train.shape[1:]), # 이 데이터는 이상치가 많기 때문에 과대적합을 막기 위해 은닉층을 한 개만 설정해줬다. \n",
    "    keras.layers.Dense(1) # 회귀는 마지막 활성화 함수가 없는 하나의 뉴런이다.\n",
    "])\n",
    "\n",
    "metrics=[keras.metrics.sparse_categorical_accuracy]\n",
    "model.compile(loss='mean_squared_error',optimizer = 'sgd')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 0s 37us/sample - loss: 1.0627 - val_loss: 0.5768\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.4862 - val_loss: 0.4869\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.4502 - val_loss: 0.4722\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.4299 - val_loss: 0.4673\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.4557 - val_loss: 0.4630\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.4281 - val_loss: 0.4324\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.4004 - val_loss: 0.4244\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.3950 - val_loss: 0.4226\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.3900 - val_loss: 0.4197\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.3842 - val_loss: 0.4292\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.3851 - val_loss: 0.4535\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.3796 - val_loss: 0.4095\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.3769 - val_loss: 0.4114\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.3725 - val_loss: 0.4130\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.3718 - val_loss: 0.4110\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.3687 - val_loss: 0.4083\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.3670 - val_loss: 0.4102\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 0s 26us/sample - loss: 0.3650 - val_loss: 0.4072\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3630 - val_loss: 0.4076\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.3618 - val_loss: 0.4128\n",
      "5160/5160 [==============================] - 0s 13us/sample - loss: 2.1166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.3597173],\n",
       "       [2.3716075],\n",
       "       [8.403187 ]], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "history = model.fit(X_train,y_train, epochs =20, validation_data=(X_valid,y_valid))\n",
    "mse_test = model.evaluate(X_test,y_test)\n",
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 함수형 API를 이용해 복잡한 모델만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full,X_test,y_train_full,y_test = train_test_split(housing['data'],housing['target'])\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full,y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.fit_transform(X_valid)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape=X_train.shape[1:]) # input 객체 만들기. \n",
    "hidden1 = keras.layers.Dense(30,activation = 'relu')(input_) # 30개의 뉴런과 ReLU 활성화 함수를 가진 Dense 층을 만든다. 입력과 함께 호출\n",
    "hidden2 = keras.layers.Dense(30,activation = 'relu')(hidden1) # 윗 줄과 동일\n",
    "concat = keras.layers.Concatenate()([input_,hidden2]) # 두 번째 층과 입력층을 연결하기\n",
    "output = keras.layers.Dense(1)(concat) # 하나의 뉴런과 활성화 함수가 없는 출력층을 만들고 concat층과 연결\n",
    "model = keras.Model(inputs = [input_],outputs = [output]) # 케라스 모델로 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape = [5],name = 'wide_input')\n",
    "input_B = keras.layers.Input(shape = [6],name = 'deep_input')\n",
    "hidden1 = keras.layers.Dense(30,activation='relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30,activation='relu')(hidden1)\n",
    "concat = keras.layers.concatenate([input_A,hidden2])\n",
    "output = keras.layers.Dense(1,name= 'output')(concat)\n",
    "model = keras.Model(inputs = [input_A,input_B],outputs = [output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = \"mse\",optimizer = keras.optimizers.SGD(lr=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 0s 41us/sample - loss: 1.9038 - val_loss: 0.8852\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.8437 - val_loss: 0.6841\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.7117 - val_loss: 0.6136\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.6568 - val_loss: 0.5715\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.6224 - val_loss: 0.5428\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.5969 - val_loss: 0.5201\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.5771 - val_loss: 0.5034\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.5597 - val_loss: 0.4892\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.5456 - val_loss: 0.4693\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.5329 - val_loss: 0.4603\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.5224 - val_loss: 0.4526\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.5116 - val_loss: 0.4576\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.5094 - val_loss: 0.4417\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 0s 26us/sample - loss: 0.4990 - val_loss: 0.4407\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 0s 26us/sample - loss: 0.4946 - val_loss: 0.4578\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 0s 26us/sample - loss: 0.4900 - val_loss: 0.4447\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.4879 - val_loss: 0.4521\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 0s 26us/sample - loss: 0.4853 - val_loss: 0.4511\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 0s 26us/sample - loss: 0.4817 - val_loss: 0.4550\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 0s 26us/sample - loss: 0.4795 - val_loss: 0.4625\n",
      "5160/5160 [==============================] - 0s 14us/sample - loss: 0.4767\n"
     ]
    }
   ],
   "source": [
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20,\n",
    "                    validation_data=((X_valid_A, X_valid_B), y_valid))\n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "y_pred = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 서브클래싱 API로 동적 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepModel(keras.Model):\n",
    "    def __init__(self, units = 30, activation = 'relu',**kargs):\n",
    "        super().__init__(**kargs)\n",
    "        self.hidden1 = keras.layers.Dense(units,activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units,activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "\n",
    "model = DeepModel(30, activation=\"relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/10\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 2.6470 - output_1_loss: 2.4159 - output_2_loss: 4.7161 - val_loss: 1.2124 - val_output_1_loss: 0.9614 - val_output_2_loss: 3.4689\n",
      "Epoch 2/10\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 1.0998 - output_1_loss: 0.8938 - output_2_loss: 2.9535 - val_loss: 0.8963 - val_output_1_loss: 0.7459 - val_output_2_loss: 2.2475\n",
      "Epoch 3/10\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.8885 - output_1_loss: 0.7551 - output_2_loss: 2.0894 - val_loss: 0.7692 - val_output_1_loss: 0.6602 - val_output_2_loss: 1.7486\n",
      "Epoch 4/10\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.7975 - output_1_loss: 0.6940 - output_2_loss: 1.7303 - val_loss: 0.7072 - val_output_1_loss: 0.6137 - val_output_2_loss: 1.5477\n",
      "Epoch 5/10\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.7478 - output_1_loss: 0.6567 - output_2_loss: 1.5719 - val_loss: 0.6664 - val_output_1_loss: 0.5789 - val_output_2_loss: 1.4526\n",
      "Epoch 6/10\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.7134 - output_1_loss: 0.6274 - output_2_loss: 1.4879 - val_loss: 0.6354 - val_output_1_loss: 0.5516 - val_output_2_loss: 1.3883\n",
      "Epoch 7/10\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.6868 - output_1_loss: 0.6042 - output_2_loss: 1.4289 - val_loss: 0.6103 - val_output_1_loss: 0.5296 - val_output_2_loss: 1.3353\n",
      "Epoch 8/10\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.6653 - output_1_loss: 0.5858 - output_2_loss: 1.3808 - val_loss: 0.5877 - val_output_1_loss: 0.5097 - val_output_2_loss: 1.2887\n",
      "Epoch 9/10\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.6472 - output_1_loss: 0.5700 - output_2_loss: 1.3400 - val_loss: 0.5684 - val_output_1_loss: 0.4929 - val_output_2_loss: 1.2461\n",
      "Epoch 10/10\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.6318 - output_1_loss: 0.5574 - output_2_loss: 1.3022 - val_loss: 0.5548 - val_output_1_loss: 0.4822 - val_output_2_loss: 1.2066\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", loss_weights=[0.9, 0.1], optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "history = model.fit((X_train_A, X_train_B), (y_train, y_train), epochs=10,\n",
    "                    validation_data=((X_valid_A, X_valid_B), (y_valid, y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160/5160 [==============================] - 0s 17us/sample - loss: 0.6270 - output_1_loss: 0.5489 - output_2_loss: 1.3278\n"
     ]
    }
   ],
   "source": [
    "total_loss, main_loss, aux_loss = model.evaluate((X_test_A, X_test_B), (y_test, y_test))\n",
    "y_pred_main, y_pred_aux = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# 데이터 들고와서 훈련, 검증, 테스트 데이터 나눠주기.\n",
    "housing = fetch_california_housing()\n",
    "X_train_full,X_test,y_train_full,y_test = train_test_split(housing['data'],housing['target'])\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full,y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.fit_transform(X_valid)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 이렇게 할 수 도 있다.\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = [28,28]),\n",
    "    keras.layers.Dense(300, activation = 'relu'),\n",
    "    keras.layers.Dense(100, activation = 'relu'),\n",
    "    keras.layers.Dense(10, activation = 'softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 0s 38us/sample - loss: 1.1254 - val_loss: 0.7641\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.4838 - val_loss: 0.9699\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.4330 - val_loss: 1.0081\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.4123 - val_loss: 1.2232\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.3982 - val_loss: 1.4731\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.3885 - val_loss: 1.6834\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.3836 - val_loss: 1.9219\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.3795 - val_loss: 2.1043\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.3767 - val_loss: 2.1545\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.3731 - val_loss: 2.3222\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.3715 - val_loss: 2.5901\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.3680 - val_loss: 2.6341\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3659 - val_loss: 2.9214\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: 0.3671 - val_loss: 2.9784\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.3626 - val_loss: 3.1392\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3598 - val_loss: 3.1924\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3577 - val_loss: 3.3496\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.3573 - val_loss: 3.3587\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.3559 - val_loss: 3.5675\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.3551 - val_loss: 3.6478\n",
      "5160/5160 [==============================] - 0s 12us/sample - loss: 0.4131\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.4480991],\n",
       "       [2.4052885],\n",
       "       [2.9039862]], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full,X_test,y_train_full,y_test = train_test_split(housing['data'],housing['target'])\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full,y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.fit_transform(X_valid)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    keras.layers.Dense(30,activation = 'relu',input_shape = X_train.shape[1:]), # 이 데이터는 이상치가 많기 때문에 과대적합을 막기 위해 은닉층을 한 개만 설정해줬다. \n",
    "    keras.layers.Dense(1) # 회귀는 마지막 활성화 함수가 없는 하나의 뉴런이다.\n",
    "])\n",
    "\n",
    "metrics=[keras.metrics.sparse_categorical_accuracy]\n",
    "model.compile(loss='mean_squared_error',optimizer = 'sgd')\n",
    "\n",
    "check_point = keras.callbacks.ModelCheckpoint('my_keras_model.h5',save_best_only=True)\n",
    "history = model.fit(X_train,y_train, epochs =20, validation_data=(X_valid,y_valid),callbacks=[check_point])\n",
    "mse_test = model.evaluate(X_test,y_test)\n",
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('my_keras_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x7fdb53e7b490>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_logdir = os.path.join(os.curdir,'test_logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime('run_%Y_%m_%d-%H_%M_%S')\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./test_logs/run_2020_11_14-20_53_26'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])    \n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/30\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 1.8721 - val_loss: 0.7241\n",
      "Epoch 2/30\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.6656 - val_loss: 0.6107\n",
      "Epoch 3/30\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.5977 - val_loss: 0.5808\n",
      "Epoch 4/30\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.5616 - val_loss: 0.5581\n",
      "Epoch 5/30\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.5335 - val_loss: 0.5408\n",
      "Epoch 6/30\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.5127 - val_loss: 0.5244\n",
      "Epoch 7/30\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.4952 - val_loss: 0.5150\n",
      "Epoch 8/30\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.4819 - val_loss: 0.5064\n",
      "Epoch 9/30\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 0.4708 - val_loss: 0.5020\n",
      "Epoch 10/30\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.4624 - val_loss: 0.4978\n",
      "Epoch 11/30\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.4549 - val_loss: 0.4963\n",
      "Epoch 12/30\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.4490 - val_loss: 0.4925\n",
      "Epoch 13/30\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.4435 - val_loss: 0.4917\n",
      "Epoch 14/30\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.4397 - val_loss: 0.4934\n",
      "Epoch 15/30\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.4359 - val_loss: 0.4954\n",
      "Epoch 16/30\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.4322 - val_loss: 0.4924\n",
      "Epoch 17/30\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.4288 - val_loss: 0.4992\n",
      "Epoch 18/30\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.4262 - val_loss: 0.5009\n",
      "Epoch 19/30\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.4233 - val_loss: 0.5072\n",
      "Epoch 20/30\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.4213 - val_loss: 0.5035\n",
      "Epoch 21/30\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.4183 - val_loss: 0.5119\n",
      "Epoch 22/30\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.4162 - val_loss: 0.5169\n",
      "Epoch 23/30\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.4138 - val_loss: 0.5157\n",
      "Epoch 24/30\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.4116 - val_loss: 0.5242\n",
      "Epoch 25/30\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.4096 - val_loss: 0.5306\n",
      "Epoch 26/30\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.4076 - val_loss: 0.5361\n",
      "Epoch 27/30\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.4056 - val_loss: 0.5461\n",
      "Epoch 28/30\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.4040 - val_loss: 0.5581\n",
      "Epoch 29/30\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.4022 - val_loss: 0.5595\n",
      "Epoch 30/30\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.4002 - val_loss: 0.5847\n"
     ]
    }
   ],
   "source": [
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[check_point, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 신경망 하이퍼파리미터 튜닝하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망의 유연성은 단점이기도 하다\n",
    "# 조절할 하이퍼 파라미터가 많기 때문이다.\n",
    "# 층의 개수, 층마다 있는 뉴런의 개수, 각 층에서 사용하는 활성화 함수, 가중치 초기화 전략 등 많은 것을 바꿀 수 있다.\n",
    "# GridSeachCV, RandomizedSearchCV를 사용해 하이퍼 파라미터 공간을 탐색할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden =1, n_neurons =30, learning_rate = 3e-3, input_shape = [8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation = 'relu'))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(lr = learning_rate)\n",
    "    model.compile(loss ='mse',optimizer = optimizer,metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "11610/11610 [==============================] - 0s 41us/sample - loss: 1.2648 - accuracy: 0.0030 - val_loss: 0.9223 - val_accuracy: 0.0028\n",
      "Epoch 2/100\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: 0.9264 - accuracy: 0.0031 - val_loss: 2.0240 - val_accuracy: 0.0028\n",
      "Epoch 3/100\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: 2.1083 - accuracy: 0.0029 - val_loss: 0.5939 - val_accuracy: 0.0028\n",
      "Epoch 4/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.5365 - accuracy: 0.0030 - val_loss: 0.5417 - val_accuracy: 0.0028\n",
      "Epoch 5/100\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: 0.4979 - accuracy: 0.0030 - val_loss: 0.5131 - val_accuracy: 0.0028\n",
      "Epoch 6/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.4770 - accuracy: 0.0030 - val_loss: 0.4905 - val_accuracy: 0.0028\n",
      "Epoch 7/100\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: 0.4623 - accuracy: 0.0028 - val_loss: 0.4790 - val_accuracy: 0.0028\n",
      "Epoch 8/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.4518 - accuracy: 0.0029 - val_loss: 0.4690 - val_accuracy: 0.0028\n",
      "Epoch 9/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.4436 - accuracy: 0.0029 - val_loss: 0.4597 - val_accuracy: 0.0028\n",
      "Epoch 10/100\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: 0.4365 - accuracy: 0.0028 - val_loss: 0.4517 - val_accuracy: 0.0028\n",
      "Epoch 11/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.4307 - accuracy: 0.0031 - val_loss: 0.4446 - val_accuracy: 0.0028\n",
      "Epoch 12/100\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: 0.4259 - accuracy: 0.0031 - val_loss: 0.4376 - val_accuracy: 0.0028\n",
      "Epoch 13/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.4214 - accuracy: 0.0031 - val_loss: 0.4324 - val_accuracy: 0.0028\n",
      "Epoch 14/100\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: 0.4177 - accuracy: 0.0031 - val_loss: 0.4288 - val_accuracy: 0.0028\n",
      "Epoch 15/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.4139 - accuracy: 0.0031 - val_loss: 0.4260 - val_accuracy: 0.0028\n",
      "Epoch 16/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.4110 - accuracy: 0.0031 - val_loss: 0.4230 - val_accuracy: 0.0028\n",
      "Epoch 17/100\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: 0.4077 - accuracy: 0.0031 - val_loss: 0.4190 - val_accuracy: 0.0028\n",
      "Epoch 18/100\n",
      "11610/11610 [==============================] - 0s 26us/sample - loss: 0.4054 - accuracy: 0.0031 - val_loss: 0.4144 - val_accuracy: 0.0028\n",
      "Epoch 19/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.4025 - accuracy: 0.0031 - val_loss: 0.4129 - val_accuracy: 0.0028\n",
      "Epoch 20/100\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: 0.3998 - accuracy: 0.0031 - val_loss: 0.4125 - val_accuracy: 0.0028\n",
      "Epoch 21/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3979 - accuracy: 0.0031 - val_loss: 0.4093 - val_accuracy: 0.0028\n",
      "Epoch 22/100\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: 0.3953 - accuracy: 0.0031 - val_loss: 0.4055 - val_accuracy: 0.0028\n",
      "Epoch 23/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3933 - accuracy: 0.0031 - val_loss: 0.4048 - val_accuracy: 0.0028\n",
      "Epoch 24/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3910 - accuracy: 0.0031 - val_loss: 0.4029 - val_accuracy: 0.0028\n",
      "Epoch 25/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3889 - accuracy: 0.0031 - val_loss: 0.4011 - val_accuracy: 0.0028\n",
      "Epoch 26/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3869 - accuracy: 0.0031 - val_loss: 0.3989 - val_accuracy: 0.0028\n",
      "Epoch 27/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3853 - accuracy: 0.0031 - val_loss: 0.3984 - val_accuracy: 0.0028\n",
      "Epoch 28/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3835 - accuracy: 0.0031 - val_loss: 0.3965 - val_accuracy: 0.0028\n",
      "Epoch 29/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3818 - accuracy: 0.0031 - val_loss: 0.3957 - val_accuracy: 0.0028\n",
      "Epoch 30/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3805 - accuracy: 0.0031 - val_loss: 0.3938 - val_accuracy: 0.0028\n",
      "Epoch 31/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3786 - accuracy: 0.0031 - val_loss: 0.3920 - val_accuracy: 0.0028\n",
      "Epoch 32/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3774 - accuracy: 0.0031 - val_loss: 0.3908 - val_accuracy: 0.0028\n",
      "Epoch 33/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3757 - accuracy: 0.0031 - val_loss: 0.3919 - val_accuracy: 0.0028\n",
      "Epoch 34/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3746 - accuracy: 0.0031 - val_loss: 0.3883 - val_accuracy: 0.0028\n",
      "Epoch 35/100\n",
      "11610/11610 [==============================] - 0s 26us/sample - loss: 0.3732 - accuracy: 0.0031 - val_loss: 0.3857 - val_accuracy: 0.0028\n",
      "Epoch 36/100\n",
      "11610/11610 [==============================] - 0s 26us/sample - loss: 0.3721 - accuracy: 0.0031 - val_loss: 0.3852 - val_accuracy: 0.0028\n",
      "Epoch 37/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3708 - accuracy: 0.0031 - val_loss: 0.3835 - val_accuracy: 0.0028\n",
      "Epoch 38/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3698 - accuracy: 0.0031 - val_loss: 0.3832 - val_accuracy: 0.0028\n",
      "Epoch 39/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3689 - accuracy: 0.0031 - val_loss: 0.3826 - val_accuracy: 0.0028\n",
      "Epoch 40/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3674 - accuracy: 0.0031 - val_loss: 0.3808 - val_accuracy: 0.0028\n",
      "Epoch 41/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3664 - accuracy: 0.0031 - val_loss: 0.3795 - val_accuracy: 0.0028\n",
      "Epoch 42/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3655 - accuracy: 0.0031 - val_loss: 0.3783 - val_accuracy: 0.0028\n",
      "Epoch 43/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3644 - accuracy: 0.0031 - val_loss: 0.3782 - val_accuracy: 0.0028\n",
      "Epoch 44/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3633 - accuracy: 0.0031 - val_loss: 0.3768 - val_accuracy: 0.0028\n",
      "Epoch 45/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3627 - accuracy: 0.0031 - val_loss: 0.3763 - val_accuracy: 0.0028\n",
      "Epoch 46/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3615 - accuracy: 0.0031 - val_loss: 0.3757 - val_accuracy: 0.0028\n",
      "Epoch 47/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3611 - accuracy: 0.0031 - val_loss: 0.3766 - val_accuracy: 0.0028\n",
      "Epoch 48/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3598 - accuracy: 0.0031 - val_loss: 0.3737 - val_accuracy: 0.0028\n",
      "Epoch 49/100\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: 0.3595 - accuracy: 0.0031 - val_loss: 0.3737 - val_accuracy: 0.0028\n",
      "Epoch 50/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3587 - accuracy: 0.0031 - val_loss: 0.3744 - val_accuracy: 0.0028\n",
      "Epoch 51/100\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: 0.3577 - accuracy: 0.0031 - val_loss: 0.3768 - val_accuracy: 0.0028\n",
      "Epoch 52/100\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: 0.3572 - accuracy: 0.0031 - val_loss: 0.3730 - val_accuracy: 0.0028\n",
      "Epoch 53/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3564 - accuracy: 0.0031 - val_loss: 0.3720 - val_accuracy: 0.0028\n",
      "Epoch 54/100\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: 0.3556 - accuracy: 0.0031 - val_loss: 0.3760 - val_accuracy: 0.0028\n",
      "Epoch 55/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3557 - accuracy: 0.0031 - val_loss: 0.3697 - val_accuracy: 0.0028\n",
      "Epoch 56/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3547 - accuracy: 0.0031 - val_loss: 0.3708 - val_accuracy: 0.0028\n",
      "Epoch 57/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3543 - accuracy: 0.0031 - val_loss: 0.3696 - val_accuracy: 0.0028\n",
      "Epoch 58/100\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: 0.3532 - accuracy: 0.0031 - val_loss: 0.3713 - val_accuracy: 0.0028\n",
      "Epoch 59/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3527 - accuracy: 0.0031 - val_loss: 0.3683 - val_accuracy: 0.0028\n",
      "Epoch 60/100\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: 0.3521 - accuracy: 0.0031 - val_loss: 0.3684 - val_accuracy: 0.0028\n",
      "Epoch 61/100\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: 0.3518 - accuracy: 0.0031 - val_loss: 0.3675 - val_accuracy: 0.0028\n",
      "Epoch 62/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3509 - accuracy: 0.0031 - val_loss: 0.3651 - val_accuracy: 0.0028\n",
      "Epoch 63/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3501 - accuracy: 0.0031 - val_loss: 0.3648 - val_accuracy: 0.0028\n",
      "Epoch 64/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3498 - accuracy: 0.0031 - val_loss: 0.3647 - val_accuracy: 0.0028\n",
      "Epoch 65/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3494 - accuracy: 0.0031 - val_loss: 0.3648 - val_accuracy: 0.0028\n",
      "Epoch 66/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3483 - accuracy: 0.0031 - val_loss: 0.3640 - val_accuracy: 0.0028\n",
      "Epoch 67/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3477 - accuracy: 0.0031 - val_loss: 0.3658 - val_accuracy: 0.0028\n",
      "Epoch 68/100\n",
      "11610/11610 [==============================] - 0s 26us/sample - loss: 0.3468 - accuracy: 0.0031 - val_loss: 0.3626 - val_accuracy: 0.0028\n",
      "Epoch 69/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3472 - accuracy: 0.0031 - val_loss: 0.3648 - val_accuracy: 0.0028\n",
      "Epoch 70/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3464 - accuracy: 0.0031 - val_loss: 0.3629 - val_accuracy: 0.0028\n",
      "Epoch 71/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3453 - accuracy: 0.0031 - val_loss: 0.3617 - val_accuracy: 0.0028\n",
      "Epoch 72/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3449 - accuracy: 0.0031 - val_loss: 0.3614 - val_accuracy: 0.0028\n",
      "Epoch 73/100\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.3442 - accuracy: 0.0031 - val_loss: 0.3611 - val_accuracy: 0.0028\n",
      "Epoch 74/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3438 - accuracy: 0.0031 - val_loss: 0.3605 - val_accuracy: 0.0028\n",
      "Epoch 75/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3433 - accuracy: 0.0031 - val_loss: 0.3592 - val_accuracy: 0.0028\n",
      "Epoch 76/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3430 - accuracy: 0.0031 - val_loss: 0.3591 - val_accuracy: 0.0028\n",
      "Epoch 77/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3420 - accuracy: 0.0031 - val_loss: 0.3595 - val_accuracy: 0.0028\n",
      "Epoch 78/100\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: 0.3428 - accuracy: 0.0031 - val_loss: 0.3581 - val_accuracy: 0.0028\n",
      "Epoch 79/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3407 - accuracy: 0.0031 - val_loss: 0.3596 - val_accuracy: 0.0028\n",
      "Epoch 80/100\n",
      "11610/11610 [==============================] - 0s 26us/sample - loss: 0.3407 - accuracy: 0.0031 - val_loss: 0.3567 - val_accuracy: 0.0028\n",
      "Epoch 81/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3404 - accuracy: 0.0031 - val_loss: 0.3613 - val_accuracy: 0.0028\n",
      "Epoch 82/100\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: 0.3403 - accuracy: 0.0031 - val_loss: 0.3592 - val_accuracy: 0.0028\n",
      "Epoch 83/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3392 - accuracy: 0.0031 - val_loss: 0.3575 - val_accuracy: 0.0028\n",
      "Epoch 84/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3390 - accuracy: 0.0031 - val_loss: 0.3560 - val_accuracy: 0.0028\n",
      "Epoch 85/100\n",
      "11610/11610 [==============================] - 0s 26us/sample - loss: 0.3384 - accuracy: 0.0031 - val_loss: 0.3565 - val_accuracy: 0.0028\n",
      "Epoch 86/100\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: 0.3376 - accuracy: 0.0031 - val_loss: 0.3562 - val_accuracy: 0.0028\n",
      "Epoch 87/100\n",
      "11610/11610 [==============================] - 0s 26us/sample - loss: 0.3378 - accuracy: 0.0031 - val_loss: 0.3548 - val_accuracy: 0.0028\n",
      "Epoch 88/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3373 - accuracy: 0.0031 - val_loss: 0.3562 - val_accuracy: 0.0028\n",
      "Epoch 89/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3373 - accuracy: 0.0031 - val_loss: 0.3546 - val_accuracy: 0.0028\n",
      "Epoch 90/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3365 - accuracy: 0.0031 - val_loss: 0.3538 - val_accuracy: 0.0028\n",
      "Epoch 91/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3362 - accuracy: 0.0031 - val_loss: 0.3537 - val_accuracy: 0.0028\n",
      "Epoch 92/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3358 - accuracy: 0.0031 - val_loss: 0.3528 - val_accuracy: 0.0028\n",
      "Epoch 93/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3348 - accuracy: 0.0031 - val_loss: 0.3524 - val_accuracy: 0.0028\n",
      "Epoch 94/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3351 - accuracy: 0.0031 - val_loss: 0.3530 - val_accuracy: 0.0028\n",
      "Epoch 95/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3342 - accuracy: 0.0031 - val_loss: 0.3535 - val_accuracy: 0.0028\n",
      "Epoch 96/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3340 - accuracy: 0.0031 - val_loss: 0.3524 - val_accuracy: 0.0028\n",
      "Epoch 97/100\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: 0.3337 - accuracy: 0.0031 - val_loss: 0.3524 - val_accuracy: 0.0028\n",
      "Epoch 98/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3332 - accuracy: 0.0031 - val_loss: 0.3536 - val_accuracy: 0.0028\n",
      "Epoch 99/100\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: 0.3329 - accuracy: 0.0031 - val_loss: 0.3512 - val_accuracy: 0.0028\n",
      "Epoch 100/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3324 - accuracy: 0.0031 - val_loss: 0.3511 - val_accuracy: 0.0028\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbf68108e90>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full,X_test,y_train_full,y_test = train_test_split(housing['data'],housing['target'])\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full,y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.fit_transform(X_valid)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\n",
    "keras_reg.fit(X_train, y_train, epochs=100,\n",
    "              validation_data=(X_valid, y_valid),\n",
    "              callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160/5160 [==============================] - 0s 13us/sample - loss: 0.7549 - accuracy: 0.0023\n"
     ]
    }
   ],
   "source": [
    "mes_test = keras_reg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
